---
title: "Handling a long runtime"
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

This article shows how to calculate an indicator when it takes too long to run.
It shows an approach that shows your progress, avoids running the same thing
twice, continues despite errors, and allows you to restart from where you left
after an interruption.

### Setup

We'll need a number of packages. 

```{r}
library(dplyr, warn.conflicts = FALSE)
library(readr, warn.conflicts = FALSE)
library(tiltIndicator)
library(fs)

options(readr.show_col_types = FALSE)
packageVersion("tiltIndicator")
```

Here I use data from a temporary directory.

```{r}
# TODO: Replace `tempdir()` with something like "~/Downloads"
input <- function(..., parent = tempdir()) path(parent, "input", ...)
output <- function(..., parent = tempdir()) path(parent, "output", ...)
```

```{r echo=FALSE}
dir_create(input())
write_csv(tiltIndicator::companies, input("companies.csv"))
write_csv(tiltIndicator::products, input("products.csv"))
```

Here I use example data from the tiltIndicator package.

```{r}
companies <- read_csv(input("companies.csv"))

isic_as_chr <- cols(isic_4digit = col_character())
co2 <- read_csv(input("products.csv"), col_types = isic_as_chr)
```

This example calculates the indicator "PCTR". Adapt it as necessary.

```{r}
# TODO: Replace with the literal string "ictr" for ICTR
indicator <- "pctr"
```

Create a folder to otput results at each level.

```{r}
dir_create(output(indicator, "product"))
dir_create(output(indicator, "company"))
```

Split the data by company. 

```{r}
companies_list <- split(companies, companies$company_id)
```

Calculate the indicator for each company at a time, saving the result to a .csv
file, and skipping companies that are already done.

```{r}
for (i in seq_along(companies_list)) {
  companies_id <- names(companies_list[i])
  product_file <- output(indicator, "product", paste0(companies_id, ".csv"))
  company_file <- output(indicator, "company", paste0(companies_id, ".csv"))

  # Skip if already done
  if (file_exists(company_file)) next()

  # Continue even if one company fails
  try({
    product <- xctr_at_product_level(companies_list[[i]], co2)
    write_csv(product, product_file)
    company <- xctr_at_company_level(product)
    write_csv(company, company_file)
  })
}
```

Monitor progress by counting the number of company files.

```{r}
length(dir_ls(output(indicator, "company")))
```

When you're done (or before) you can read all the .csv files into a single
dataset at once.

```{r}
result <- read_csv(dir_ls(output(indicator, "company")))
result
```

If the number of files is too large, the code above will fail and you'll need to
combine results in chunks.

### Combine results in chunks

Put the paths to each file into a table, and create a column to group them in
any number of chunks. Each chunk should have no more files than you can
successfully read at once. Here I show it for company files. Adapt it for
product files.

```{r}
chunks <- 3
chunked <- tibble(file = dir_ls(output(indicator, "company"))) |>
  mutate(chunk = as.integer(cut(row_number(), chunks)))
chunked

chunked |> count(chunk)
```

Now read all the individual .csv files of a chunk into a single dataset, then
save it into a single .csv.

```{r}
dir_create(output("company_chunks"))
for (i in unique(chunked$chunk)) {
  chunk_csv <- output("company_chunks", paste0(i, ".csv"))
  if (file_exists(chunk_csv)) next()

  chunk_files <- chunked |>
    filter(chunk == i) |>
    pull(file)
  chunk_data <- read_csv(chunk_files)
  write_csv(chunk_data, chunk_csv)
}
```

The data is still split in pieces but they should be few enough to read them all
at once into a single table.

```{r}
all_companies <- read_csv(dir_ls(output("company_chunks")))
all_companies
```

### Background jobs

You may want to run this on as a [background job in
RStudio](https://docs.posit.co/ide/user/ide/guide/tools/jobs.html) so you can
use your R session for something else while the process runs on the background.

RStudio may have its own issues. If your code is in "~/projects/ictr/run.R" you
may run it directly from the terminal with:

```bash
Rscript ~/projects/ictr/run.R
```

Best is to do this from a remote server, which gives you a stable environment
and the ability to briefly rent a computer more powerful than the one you have.

### Targets

If you run long processes often you should learn about
[targets](https://docs.ropensci.org/targets/).
