---
title: "Handling a long runtime"
editor_options: 
  chunk_output_type: console
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

This article shows how to calculate an indicator when the normal approach
saturates the computer memory or takes too long to run. 

In general, a number of techniques help overcome the challenge:

* Split data and computations in chunks.
* Distribute chunks across multiple workers running in parallel.
* Store chunk results in disk -- not in memory.
* Avoid re-running stored chunks.
* Monitor the progress.

All those features come out-of-the-box in the R package
[targets](https://docs.ropensci.org/targets/), or other [pipeline tools in and
beyond R](https://github.com/pditommaso/awesome-pipeline).

Learning pipeline tools pays off but it isn't trivial, so here you'll use a
basic, custom toolkit based on tools you already know -- mostly the tidyverse
and friends.

You'll start setting up packages, options and helpers. Then you'll run an
indicator in a compact way and without explanation, to see that the entire
workflow is short. Finally you'll run a second indicator in a bit longer way and
with a more gentle explanation, to better understand what's going on.

### Setup

```{r}
library(dplyr, warn.conflicts = FALSE)
library(readr, warn.conflicts = FALSE)
library(tidyr, warn.conflicts = FALSE)
library(purrr, warn.conflicts = FALSE)
library(rappdirs)
library(future)
library(furrr)
library(fs)
library(tiltIndicator)

options(readr.show_col_types = FALSE)
# Enable computing over multiple workers in parallel
plan(multisession)

# Helpers ----

cache_path <- function(..., parent = cache_dir()) {
  path(parent, ...)
}

cache_dir <- function() {
  user_cache_dir(appname = "tiltIndicator")
}

job_pmap <- function(job, .f) {
  job |>
    pick_undone() |>
    select(data, file) |>
    future_pwalk(.f, .progress = TRUE)

  map_df(istr_job$file, read_rds)
}

nest_chunk <- function(data, .by, chunks) {
  data |>
    nest(.by = all_of(.by)) |>
    mutate(data, chunk = as.integer(cut(row_number(), chunks))) |>
    unnest(data) |>
    nest(.by = chunk)
}

add_file <- function(data, parent = cache_path(), ext = ".rds") {
  dir_create(parent)
  mutate(data, file = path(parent, paste0(chunk, ext)))
}

pick_undone <- function(data) {
  data |>
    add_done() |>
    filter(!done)
}

add_done <- function(data, file) {
  mutate(data, done = file_exists(file))
}
```

### ISTR

```{r}
# TODO: Replace with `read_csv("/path/to/input/companies.csv")`
companies <- tiltIndicator::istr_companies
# TODO: Replace with `read_csv("/path/to/input/scenarios.csv")`
scenarios <- tiltIndicator::xstr_scenarios
# TODO: Replace with `read_csv("/path/to/input/inputs.csv")`
inputs <- tiltIndicator::istr_inputs

# Create a "job" data frame where each row is a chunk of data
istr_job <- companies |>
  nest_chunk(.by = "company_id", chunks = 3) |>
  add_file(parent = cache_path("istr"))

# Chunks of data will be distributed across workers and saved to a file
istr_job

# Run each indicator chunk across multiple workers and output a combined result
istr_result <- istr_job |>
  job_pmap(\(data, file) write_rds(sector_profile_upstream(data, scenarios, inputs), file))

# TODO: `... |> write_csv("/path/to/output/product.csv")`
istr_result |> unnest_product()

# TODO: `... |> write_csv("/path/to/output/company.csv")`
istr_result |> unnest_company()

# Each chunk result was saved to a file
dir_tree(cache_path("istr"))
```

### PCTR

Read `companies` data, and define chunks and files where to later save them.

```{r}
# TODO: Replace with `read_csv("/path/to/input/companies.csv")`
companies <- tiltIndicator::companies

# TODO: Experiment to find the number of chunks that works best for you
pctr_job <- companies |>
  nest_chunk(.by = "company_id", chunks = 3) |>
  add_file(parent = cache_path("xctr"))

# `nest_chunk()` ensures all rows of a company fall in the same chunk
slice(pctr_job, 1) |> unnest(data)

slice(pctr_job, 2) |> unnest(data)
```

There is a lot going on here:

* Define a function that runs the indicator with `data` (companies) and writes it
to a `file`.
* The additional dataset `products` won't be passed through the
function but rather accessed through the global environment.
* Skip any chunk that is already saved (from a previous, incomplete run).
* Select the columns `data` and `file` so they match the names of the `*_rds()`.
This allows to use `*_pmap()` in a very succinct way.

```{r}
# TODO: Replace with `read_csv("/path/to/input/products.csv")`
products <- tiltIndicator::products

pctr_rds <- function(data, file) write_rds(emissions_profile(data, products), file)

pctr_job |>
  # Skip what's already done (if anything)
  pick_undone() |>
  # `select(data, file)` matches `pctr_rds(data, file)` to use `*_pwalk()`
  select(data, file) |>
  # Combined with `plan()` it distributes computations across multiple workers
  # The progress bar won't appear in this .Rmd document.
  future_pwalk(pctr_rds, .progress = TRUE)
```

Here is what you accomplished. Instead of saturating the memory you stored each
chunk into a file.

```{r}
dir_tree(cache_path("xctr"))
```

You can now read all the saved files at once. If the results are too many you
may also need to do this in batches or work with [multi-file
datasets](https://arrow.apache.org/docs/r/reference/open_dataset.html).

```{r}
pctr_result <- map_df(pctr_job$file, read_rds)
pctr_result
```

Typically you'll finish by writing one "*.csv" file with results at product
level, and another one with results at company level.

```{r}
# TODO: `... |> write_csv("/path/to/output/product.csv")`
pctr_result |> unnest_product()

# TODO: `... |> write_csv("/path/to/output/company.csv")`
pctr_result |> unnest_company()
```

Before the next time you run the same indicator you need to cleanup.

```{r}
# WARNING: Deleting the hard-earned .rds files
cache_path() |>
  dir_ls(recurse = TRUE, type = "file") |>
  file_delete()
```

### Background jobs

You may want to run this on as a [background job in
RStudio](https://docs.posit.co/ide/user/ide/guide/tools/jobs.html) so you can
use your R session for something else while the process runs on the background.

RStudio may have its own issues. If your code is in "~/projects/run.R" you
may run it directly from the terminal with: `Rscript ~/projects/run.R`.

Best is to do this from a remote server, which gives you a stable environment
and the ability to briefly rent a computer more powerful than the one you have.
