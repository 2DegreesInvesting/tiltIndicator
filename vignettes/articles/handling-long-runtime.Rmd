---
title: "Handling a long runtime"
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

This article shows how to calculate an indicator when it takes too long to run.
It shows an approach that shows your progress, avoids running the same thing
twice, continues despite errors, and allows you to restart from where you left
after an interruption.

### Setup

We'll need a number of packages. 

```{r}
library(dplyr, warn.conflicts = FALSE)
library(readr, warn.conflicts = FALSE)
library(tiltIndicator)
library(fs)

options(readr.show_col_types = FALSE)
packageVersion("tiltIndicator")
```

Here I use data from a temporary directory.

```{r}
# TODO: Replace `tempdir()` with something like "~/Downloads"
input <- function(..., parent = tempdir()) path(parent, "input", ...)
output <- function(..., parent = tempdir()) path(parent, "output", ...)
```

```{r echo=FALSE}
dir_create(input())
write_csv(tiltIndicator::companies, input("companies.csv"))
write_csv(tiltIndicator::products, input("products.csv"))
```

Here I use example data from the tiltIndicator package.

```{r}
real_companies <- read_csv(input("companies.csv"))

isic_4digit_as_chr <- cols(isic_4digit = col_character())
real_co2 <- read_csv(input("products.csv"), col_types = isic_4digit_as_chr)
```

This example calculates the indicator "PCTR". Adapt it as necessary.

```{r}
# TODO: Replace with the literal string "ictr" for ICTR
indicator <- "pctr"

# Create a folder to store the results of each indicator
dir_create(output(indicator, "product"))
dir_create(output(indicator, "company"))
```

Split the data by company. 

```{r}
companies_list <- split(real_companies, real_companies$company_id)
```

Calculate the indicator for each company at a time, saving the result to a .csv
file, and skipping companies that are already done.

```{r}
for (i in seq_along(companies_list)) {
  companies_id <- names(companies_list[i])
  product_file <- output(indicator, "product", paste0(companies_id, ".csv"))
  company_file <- output(indicator, "company", paste0(companies_id, ".csv"))
  # Skip if run previously
  if (file_exists(company_file)) next()
  
  # Continue even if one company fails
  try({
    product <- xctr_at_product_level(companies_list[[i]], real_co2)
    write_csv(product, product_file)
    company <- xctr_at_company_level(product)
    write_csv(company, company_file)
  })
}
```

Monitor progress by counting the number of company files.

```{r}
length(dir_ls(output(indicator, "company")))
```

When you're done (or before) you can read all the .csv files into a single
dataset at once.

```{r}
combined_results <- read_csv(dir_ls(output(indicator, "company")))
combined_results
```

If the number of files is too large, the code above will fail and you'll need to
combine them in chunks, then read all combined chunks at once.

### Combine chunks of files

Consider the files at company level. 

We can put the paths to each file into a table, and create a column to group
them in any number of chunks. Each chunk should have no more files than you can
successfully read at once.

```{r}
chunks <- 3
chunked <- tibble(file = dir_ls(output(indicator, "company"))) |> 
  mutate(chunk = as.integer(cut(row_number(), chunks)))
chunked

chunked |> count(chunk)
```

Now read each chunk of file at once, and save them into a single combined .csv.

```{r}
dir_create(output("company_chunks"))
for (i in unique(chunked$chunk)) {
  chunk_csv <- output("company_chunks", paste0(i, ".csv"))
  if (file_exists(chunk_csv)) next()

  chunk_files <- chunked |> filter(chunk == i) |> pull(file)
  chunk_data <- read_csv(chunk_files)
  write_csv(chunk_data, chunk_csv)
}
```

Finally the number of chunk files should be small enough to read them at once.

```{r}
result <- read_csv(dir_ls(output("company_chunks")))
result
```

### Background jobs

You may want to run this on as a [background
job](https://docs.posit.co/ide/user/ide/guide/tools/jobs.html) so you can use
your R session for something else while the process runs on the background. Or
better, you may run it on a remote server so you can rent a massive computer for
a short time, and use your time and laptop for other things.
